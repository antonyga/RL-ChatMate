What is tokenization?

Tokenization splits text into individual tokens (e.g., words, punctuation marks, or subword units).
Itâ€™s essential for converting raw text into a format suitable for machine learning models.

Source:
https://sl.bing.net/dVwXXdEDZCu
